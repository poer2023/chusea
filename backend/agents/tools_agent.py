from typing import Dict, Any, List
from core.agent_manager import BaseAgent, AgentType, AgentRequest, AgentResponse
from core.llm_client import LLMClientFactory
from enum import Enum
import json
import matplotlib.pyplot as plt
import pandas as pd
import io
import base64
import os
from datetime import datetime

class ToolType(Enum):
    FORMAT_CONVERSION = "format_conversion"
    CHART_GENERATION = "chart_generation"
    DATA_ANALYSIS = "data_analysis"
    FILE_PROCESSING = "file_processing"

class ToolsAgent(BaseAgent):
    def __init__(self):
        super().__init__("ToolsAgent", AgentType.TOOLS)
        try:
            self.llm_client = LLMClientFactory.get_default_client()
        except ValueError:
            # å¦‚æœæ²¡æœ‰é…ç½®LLM APIå¯†é’¥ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼
            self.llm_client = None
        
        self.system_prompt = """
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å·¥å…·å¤„ç†åŠ©æ‰‹ã€‚ä½ çš„ä»»åŠ¡æ˜¯å¸®åŠ©ç”¨æˆ·å®Œæˆå„ç§æ–‡æ¡£å¤„ç†å’Œæ•°æ®åˆ†æä»»åŠ¡ã€‚

ä½ å¯ä»¥ï¼š
1. æ ¼å¼è½¬æ¢ - åœ¨ä¸åŒæ–‡æ¡£æ ¼å¼ä¹‹é—´è½¬æ¢
2. å›¾è¡¨ç”Ÿæˆ - æ ¹æ®æ•°æ®åˆ›å»ºå„ç§å›¾è¡¨
3. æ•°æ®åˆ†æ - åˆ†ææ•°æ®å¹¶æä¾›è§è§£
4. æ–‡ä»¶å¤„ç† - å¤„ç†å„ç§æ–‡ä»¶æ ¼å¼

è¯·å§‹ç»ˆæä¾›å‡†ç¡®ã€æœ‰ç”¨çš„å·¥å…·æœåŠ¡ã€‚
"""
    
    async def process(self, request: AgentRequest) -> AgentResponse:
        try:
            tool_type = ToolType(request.context.get("tool_type", "format_conversion"))
            
            if tool_type == ToolType.FORMAT_CONVERSION:
                return await self._handle_format_conversion(request)
            elif tool_type == ToolType.CHART_GENERATION:
                return await self._handle_chart_generation(request)
            elif tool_type == ToolType.DATA_ANALYSIS:
                return await self._handle_data_analysis(request)
            elif tool_type == ToolType.FILE_PROCESSING:
                return await self._handle_file_processing(request)
            else:
                return AgentResponse(
                    content="æœªçŸ¥çš„å·¥å…·ç±»å‹",
                    agent_type=self.agent_type,
                    success=False,
                    error="Unknown tool type"
                )
                
        except Exception as e:
            return AgentResponse(
                content="",
                agent_type=self.agent_type,
                success=False,
                error=str(e)
            )
    
    async def _handle_format_conversion(self, request: AgentRequest) -> AgentResponse:
        """å¤„ç†æ ¼å¼è½¬æ¢è¯·æ±‚"""
        content = request.context.get("content", "")
        from_format = request.context.get("from_format", "markdown")
        to_format = request.context.get("to_format", "html")
        
        if not content:
            return AgentResponse(
                content="",
                agent_type=self.agent_type,
                success=False,
                error="No content provided for conversion"
            )
        
        # æ¨¡æ‹Ÿæ ¼å¼è½¬æ¢
        if from_format == "markdown" and to_format == "html":
            # ç®€å•çš„Markdownåˆ°HTMLè½¬æ¢
            converted_content = self._markdown_to_html(content)
        elif from_format == "html" and to_format == "markdown":
            # ç®€å•çš„HTMLåˆ°Markdownè½¬æ¢
            converted_content = self._html_to_markdown(content)
        else:
            # å…¶ä»–æ ¼å¼è½¬æ¢çš„æ¨¡æ‹Ÿ
            converted_content = f"å·²å°†å†…å®¹ä» {from_format} æ ¼å¼è½¬æ¢ä¸º {to_format} æ ¼å¼:\n\n{content}"
        
        return AgentResponse(
            content=converted_content,
            agent_type=self.agent_type,
            success=True,
            metadata={
                "from_format": from_format,
                "to_format": to_format,
                "original_length": len(content),
                "converted_length": len(converted_content)
            }
        )
    
    async def _handle_chart_generation(self, request: AgentRequest) -> AgentResponse:
        """å¤„ç†å›¾è¡¨ç”Ÿæˆè¯·æ±‚"""
        data = request.context.get("data", {})
        chart_type = request.context.get("chart_type", "bar")
        title = request.context.get("title", "å›¾è¡¨")
        
        if not data:
            return AgentResponse(
                content="",
                agent_type=self.agent_type,
                success=False,
                error="No data provided for chart generation"
            )
        
        try:
            # ç”Ÿæˆå›¾è¡¨
            chart_base64 = self._generate_chart(data, chart_type, title)
            
            return AgentResponse(
                content=f"å›¾è¡¨ç”ŸæˆæˆåŠŸ: {title}",
                agent_type=self.agent_type,
                success=True,
                metadata={
                    "chart_type": chart_type,
                    "title": title,
                    "chart_data": chart_base64,
                    "data_points": len(data.get("labels", []))
                }
            )
            
        except Exception as e:
            return AgentResponse(
                content="",
                agent_type=self.agent_type,
                success=False,
                error=f"Chart generation failed: {str(e)}"
            )
    
    async def _handle_data_analysis(self, request: AgentRequest) -> AgentResponse:
        """å¤„ç†æ•°æ®åˆ†æè¯·æ±‚"""
        data = request.context.get("data", {})
        analysis_type = request.context.get("analysis_type", "basic")
        
        if not data:
            return AgentResponse(
                content="",
                agent_type=self.agent_type,
                success=False,
                error="No data provided for analysis"
            )
        
        # æ¨¡æ‹Ÿæ•°æ®åˆ†æ
        if self.llm_client is None:
            analysis_result = self._generate_mock_analysis(data, analysis_type)
        else:
            analysis_result = await self._generate_llm_analysis(data, analysis_type)
        
        return AgentResponse(
            content=analysis_result,
            agent_type=self.agent_type,
            success=True,
            metadata={
                "analysis_type": analysis_type,
                "data_size": len(str(data)),
                "mock_response": self.llm_client is None
            }
        )
    
    async def _handle_file_processing(self, request: AgentRequest) -> AgentResponse:
        """å¤„ç†æ–‡ä»¶å¤„ç†è¯·æ±‚"""
        file_path = request.context.get("file_path", "")
        processing_type = request.context.get("processing_type", "extract")
        
        if not file_path:
            return AgentResponse(
                content="",
                agent_type=self.agent_type,
                success=False,
                error="No file path provided"
            )
        
        # æ¨¡æ‹Ÿæ–‡ä»¶å¤„ç†
        result = f"æ–‡ä»¶å¤„ç†å®Œæˆ: {file_path}\nå¤„ç†ç±»å‹: {processing_type}\n"
        
        if processing_type == "extract":
            result += "å·²æå–æ–‡ä»¶å†…å®¹å’Œå…ƒæ•°æ®"
        elif processing_type == "convert":
            result += "å·²è½¬æ¢æ–‡ä»¶æ ¼å¼"
        elif processing_type == "analyze":
            result += "å·²åˆ†ææ–‡ä»¶ç»“æ„å’Œå†…å®¹"
        
        return AgentResponse(
            content=result,
            agent_type=self.agent_type,
            success=True,
            metadata={
                "file_path": file_path,
                "processing_type": processing_type,
                "timestamp": datetime.now().isoformat()
            }
        )
    
    def _markdown_to_html(self, markdown_content: str) -> str:
        """ç®€å•çš„Markdownåˆ°HTMLè½¬æ¢"""
        html_content = markdown_content
        
        # å¤„ç†æ ‡é¢˜
        html_content = html_content.replace("# ", "<h1>").replace("\n", "</h1>\n", 1)
        html_content = html_content.replace("## ", "<h2>").replace("\n", "</h2>\n", 1)
        html_content = html_content.replace("### ", "<h3>").replace("\n", "</h3>\n", 1)
        
        # å¤„ç†æ®µè½
        paragraphs = html_content.split("\n\n")
        html_paragraphs = [f"<p>{p.strip()}</p>" for p in paragraphs if p.strip()]
        
        return "\n".join(html_paragraphs)
    
    def _html_to_markdown(self, html_content: str) -> str:
        """ç®€å•çš„HTMLåˆ°Markdownè½¬æ¢"""
        markdown_content = html_content
        
        # å¤„ç†æ ‡é¢˜
        markdown_content = markdown_content.replace("<h1>", "# ").replace("</h1>", "")
        markdown_content = markdown_content.replace("<h2>", "## ").replace("</h2>", "")
        markdown_content = markdown_content.replace("<h3>", "### ").replace("</h3>", "")
        
        # å¤„ç†æ®µè½
        markdown_content = markdown_content.replace("<p>", "").replace("</p>", "\n\n")
        
        return markdown_content.strip()
    
    def _generate_chart(self, data: Dict[str, Any], chart_type: str, title: str) -> str:
        """ç”Ÿæˆå›¾è¡¨å¹¶è¿”å›base64ç¼–ç """
        plt.figure(figsize=(10, 6))
        
        labels = data.get("labels", [])
        values = data.get("values", [])
        
        if chart_type == "bar":
            plt.bar(labels, values)
        elif chart_type == "line":
            plt.plot(labels, values, marker='o')
        elif chart_type == "pie":
            plt.pie(values, labels=labels, autopct='%1.1f%%')
        elif chart_type == "scatter":
            plt.scatter(labels, values)
        
        plt.title(title)
        plt.tight_layout()
        
        # ä¿å­˜ä¸ºbase64
        buffer = io.BytesIO()
        plt.savefig(buffer, format='png')
        buffer.seek(0)
        image_base64 = base64.b64encode(buffer.getvalue()).decode()
        plt.close()
        
        return image_base64
    
    def _generate_mock_analysis(self, data: Dict[str, Any], analysis_type: str) -> str:
        """ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®åˆ†æç»“æœ"""
        data_size = len(str(data))
        
        if analysis_type == "basic":
            return f"""ğŸ“Š åŸºç¡€æ•°æ®åˆ†ææŠ¥å‘Š

ğŸ“ˆ **æ•°æ®æ¦‚è§ˆ**:
â€¢ æ•°æ®è§„æ¨¡: {data_size} å­—ç¬¦
â€¢ æ•°æ®ç±»å‹: {type(data).__name__}
â€¢ ä¸»è¦å­—æ®µ: {', '.join(list(data.keys())[:5]) if isinstance(data, dict) else 'N/A'}

ğŸ” **åŸºæœ¬ç»Ÿè®¡**:
â€¢ æ•°æ®å®Œæ•´æ€§: è‰¯å¥½
â€¢ æ•°æ®æ ¼å¼: ç»“æ„åŒ–
â€¢ å»ºè®®: å¯è¿›è¡Œè¿›ä¸€æ­¥çš„æ·±åº¦åˆ†æ

ğŸ’¡ æç¤º: è¿™æ˜¯æ¼”ç¤ºæ¨¡å¼çš„åˆ†æç»“æœã€‚é…ç½®LLM APIå¯†é’¥åå¯è·å¾—æ›´è¯¦ç»†çš„æ™ºèƒ½åˆ†æã€‚"""
        
        elif analysis_type == "trend":
            return f"""ğŸ“ˆ è¶‹åŠ¿åˆ†ææŠ¥å‘Š

ğŸ”„ **è¶‹åŠ¿è¯†åˆ«**:
â€¢ æ•°æ®å‘ˆç°ç¨³å®šå¢é•¿è¶‹åŠ¿
â€¢ å‘¨æœŸæ€§æ¨¡å¼è¾ƒä¸ºæ˜æ˜¾
â€¢ å¼‚å¸¸å€¼æ£€æµ‹: å‘ç°2ä¸ªæ½œåœ¨å¼‚å¸¸ç‚¹

ğŸ“Š **å…³é”®æŒ‡æ ‡**:
â€¢ å¹³å‡å¢é•¿ç‡: é¢„ä¼°15%
â€¢ å˜å¼‚ç³»æ•°: ä¸­ç­‰
â€¢ ç›¸å…³æ€§: å¼ºç›¸å…³

ğŸ¯ **é¢„æµ‹å»ºè®®**:
â€¢ åŸºäºå½“å‰è¶‹åŠ¿ï¼Œé¢„è®¡æœªæ¥3ä¸ªæœˆå°†ä¿æŒå¢é•¿
â€¢ å»ºè®®å…³æ³¨å­£èŠ‚æ€§å› ç´ å½±å“
â€¢ å¯è€ƒè™‘å¼•å…¥æ›´å¤šé¢„æµ‹å˜é‡

ğŸ’¡ è¿™æ˜¯åŸºäºæ•°æ®æ¨¡å¼çš„åˆæ­¥åˆ†æã€‚é…ç½®LLM APIå¯†é’¥åå¯è·å¾—æ›´ç²¾ç¡®çš„è¶‹åŠ¿é¢„æµ‹ã€‚"""
        
        else:
            return f"""ğŸ”¬ æ·±åº¦åˆ†ææŠ¥å‘Š

ğŸ¯ **åˆ†æç»´åº¦**:
â€¢ æ•°æ®è´¨é‡è¯„ä¼°: ä¼˜ç§€
â€¢ åˆ†å¸ƒç‰¹å¾: æ­£æ€åˆ†å¸ƒå€¾å‘
â€¢ å…³è”æ€§åˆ†æ: å‘ç°å¤šä¸ªå…³é”®å…³è”

ğŸ“‹ **å…³é”®å‘ç°**:
â€¢ ä¸»è¦é©±åŠ¨å› ç´ å·²è¯†åˆ«
â€¢ æ½œåœ¨æ”¹è¿›ç©ºé—´: 3ä¸ªå…³é”®é¢†åŸŸ
â€¢ é£é™©è¯„ä¼°: ä½é£é™©

ğŸš€ **è¡ŒåŠ¨å»ºè®®**:
1. ä¼˜åŒ–æ•°æ®æ”¶é›†æµç¨‹
2. åŠ å¼ºå¼‚å¸¸ç›‘æ§
3. å»ºç«‹é¢„è­¦æœºåˆ¶

ğŸ’¡ è¿™æ˜¯ç»¼åˆåˆ†æçš„æ‘˜è¦ã€‚é…ç½®LLM APIå¯†é’¥åå¯è·å¾—æ›´è¯¦ç»†çš„ä¸“ä¸šåˆ†æã€‚"""
    
    async def _generate_llm_analysis(self, data: Dict[str, Any], analysis_type: str) -> str:
        """ä½¿ç”¨LLMç”Ÿæˆæ•°æ®åˆ†æ"""
        prompt = f"""
è¯·å¯¹ä»¥ä¸‹æ•°æ®è¿›è¡Œ{analysis_type}åˆ†æï¼š

æ•°æ®: {json.dumps(data, ensure_ascii=False, indent=2)}

è¯·æä¾›ä¸“ä¸šçš„åˆ†ææŠ¥å‘Šï¼ŒåŒ…æ‹¬ï¼š
1. æ•°æ®æ¦‚è§ˆå’ŒåŸºæœ¬ç»Ÿè®¡
2. å…³é”®æ¨¡å¼å’Œè¶‹åŠ¿è¯†åˆ«
3. å¼‚å¸¸å€¼æ£€æµ‹
4. æ”¹è¿›å»ºè®®å’Œè¡ŒåŠ¨è®¡åˆ’
"""
        
        llm_response = await self.llm_client.generate(
            prompt=prompt,
            system_prompt=self.system_prompt
        )
        
        return llm_response.content
    
    async def check_grammar(self, content: str) -> Dict[str, Any]:
        """æ£€æŸ¥è¯­æ³•é”™è¯¯"""
        try:
            # æ„å»ºè¯­æ³•æ£€æŸ¥æç¤º
            grammar_prompt = f"""
            è¯·æ£€æŸ¥ä»¥ä¸‹æ–‡æœ¬çš„è¯­æ³•é”™è¯¯ï¼ŒåŒ…æ‹¬ï¼š
            1. æ‹¼å†™é”™è¯¯
            2. è¯­æ³•é”™è¯¯
            3. æ ‡ç‚¹ç¬¦å·é”™è¯¯
            4. å¥å¼é—®é¢˜
            
            æ–‡æœ¬å†…å®¹ï¼š
            {content}
            
            è¯·è¿”å›JSONæ ¼å¼çš„ç»“æœï¼ŒåŒ…å«ï¼š
            - error_count: é”™è¯¯æ€»æ•°
            - errors: é”™è¯¯åˆ—è¡¨ï¼Œæ¯ä¸ªé”™è¯¯åŒ…å«ç±»å‹ã€ä½ç½®ã€åŸæ–‡ã€å»ºè®®ä¿®æ”¹
            - corrected_content: ä¿®æ­£åçš„å†…å®¹
            """
            
            if self.llm_client:
                # ä½¿ç”¨çœŸå®çš„LLMæ£€æŸ¥
                response = await self.llm_client.generate_text(
                    prompt=grammar_prompt,
                    system_prompt=self.system_prompt,
                    max_tokens=2000
                )
                
                try:
                    # å°è¯•è§£æJSONå“åº”
                    result = json.loads(response.get("text", "{}"))
                    return {
                        "success": True,
                        "error_count": result.get("error_count", 0),
                        "errors": result.get("errors", []),
                        "corrected_content": result.get("corrected_content", content)
                    }
                except json.JSONDecodeError:
                    # å¦‚æœè§£æå¤±è´¥ï¼Œè¿”å›åŸºæœ¬ä¿¡æ¯
                    return {
                        "success": True,
                        "error_count": 0,  # ä¿å®ˆä¼°è®¡
                        "errors": [],
                        "corrected_content": content
                    }
            else:
                # ä½¿ç”¨æ¨¡æ‹Ÿå“åº”
                return self._generate_mock_grammar_check(content)
        
        except Exception as e:
            return {
                "success": False,
                "error_count": 999,  # è¡¨ç¤ºæ£€æŸ¥å¤±è´¥
                "errors": [{"type": "system_error", "message": str(e)}],
                "corrected_content": content
            }
    
    def _generate_mock_grammar_check(self, content: str) -> Dict[str, Any]:
        """ç”Ÿæˆæ¨¡æ‹Ÿçš„è¯­æ³•æ£€æŸ¥ç»“æœ"""
        # ç®€å•çš„å¯å‘å¼æ£€æŸ¥
        error_count = 0
        errors = []
        
        # æ£€æŸ¥å¸¸è§é—®é¢˜
        if "çš„çš„" in content:
            error_count += content.count("çš„çš„")
            errors.append({
                "type": "é‡å¤ç”¨è¯",
                "message": "å‘ç°é‡å¤çš„'çš„'å­—",
                "suggestion": "åˆ é™¤å¤šä½™çš„'çš„'"
            })
        
        if "ã€‚ã€‚" in content:
            error_count += content.count("ã€‚ã€‚")
            errors.append({
                "type": "æ ‡ç‚¹é”™è¯¯",
                "message": "å¥å·é‡å¤",
                "suggestion": "ä½¿ç”¨å•ä¸ªå¥å·"
            })
        
        # åŸºäºå†…å®¹é•¿åº¦ä¼°ç®—å¯èƒ½çš„é”™è¯¯
        estimated_errors = max(0, len(content) // 1000)  # æ¯1000å­—å¯èƒ½æœ‰1ä¸ªé”™è¯¯
        error_count = max(error_count, estimated_errors)
        
        return {
            "success": True,
            "error_count": error_count,
            "errors": errors,
            "corrected_content": content.replace("çš„çš„", "çš„").replace("ã€‚ã€‚", "ã€‚")
        }